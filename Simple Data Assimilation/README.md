

**User Guide: Simple Data Assimilation**

This user guide provides step-by-step instructions on how to implement a simple data assimilation using techniques such as the Kalman Filtering to combine observational data with Numerical Weather Prediction (Model) data.  

**What is Data Assimilation**

Data assimilation (DA) refers to a class of techniques that lie at the interface between
computational sciences and real measurements, and aim at fusing information from both sides to
provide better estimates of the system‚Äôs state. One of the very mature applications that significantly
utilize DA is weather forecast, that we rely on in our daily life. In order to predict the weather (or the
state of any system) in the future, a model has to be solved, most often by numerical simulations.
However, a few problems rise at this point and we refer to only few of them here. First, for accurate
predictions, these simulations need to be initiated from the true initial condition, which is never known
exactly. For large scale systems, it is almost impossible to experimentally measure the full state of the
system at a given time. For example, imagine simulating the atmospheric or oceanic flow, then you need
to measure the velocity, temperature, density, etc. at every location corresponding to your numerical
grid! Even in the hypothetical case when this is possible, measurements are always contaminated
by noise, reducing the fidelity of your estimation. Second, the mathematical model that completely
describes all the underlying processes and dynamics of the system is either unknown or hard to
deal with. Then, approximate and simplified models are adopted instead. Third, the computational
resources always constrain the level of accuracy in the employed schemes and enforce numerical
approximations. Luckily, DA appears at the intersection of all these efforts and introduces a variety
of approaches to mitigate these problem, or at least reduce their effects. In particular, DA techniques
combines possibly incomplete dynamical models, prior information about initial system‚Äôs state and
parameterization, and sparse and corrupted measurement data to yield optimized trajectory in order
to describe the system‚Äôs dynamics and evolution.

**Kalman's Filtering**
The idea behind Kalman filtering techniques is to propagate the mean as well as the covariance
matrix of the system‚Äôs state sequentially in time. That is, in addition to providing an improved state
estimate (i.e., the analysis), it also gives some information about the statistical properties of this state
estimate. This is one main difference between Kalman filtering and variational methods, which often
assumes a fixed (stationary) background covariance matrices. Kalman filters are also very popular in
systems engineering, robotics, navigation, and control. Almost all modern control systems use the
Kalman filter. It assisted the guidance of the Apollo 11 lunar module to the moon‚Äôs surface, and most
probably will do the same for next generations of aircraft as well.
Although most application in fluid dynamics involve nonlinear systems, we first describe the
standard Kalman filter developed for the linear dynamical system case with linear observation operator
described as
```
u‚Çú(t‚Çñ+1) = M‚Çñu‚Çú(t‚Çñ) + Œæ p(t‚Çñ+1)
w(t‚Çñ) = H‚Çñu‚Çú(t‚Çñ) + Œæm(t‚Çñ)
```


where M ‚àà ‚Ñù‚ÅøÀ£‚Åø is a non-singular system matrix defining the underlying governing processes and Œæ·µñ ‚àà ‚Ñù‚Åø describes the process noise (or model error). 
H ‚àà ‚Ñù·µêÀ£‚Åø represents the measurement system with a measurement noise of Œæ·µê ‚àà ‚Ñù·µê.

As presented in Section 2, the true state u‚Çú(t‚Çñ) is assumed to be a random variable with known mean E[u‚Çú(t‚Çñ)] = ≈´(t‚Çñ) 
and covariance matrix of E[(u‚Çú(t‚Çñ) ‚àí ≈´(t‚Çñ))(u‚Çú(t‚Çñ) ‚àí ≈´(t‚Çñ))·µÄ] = B‚Çñ.

In Kalman filtering, we note that the covariance matrix evolves in time, and thus appears the subscript. 
We also assume that the process noise is unbiased with zero mean and a covariance matrix Q. 
That is E(Œæ·µñ(t‚Çñ)) = 0 and E(Œæ·µñ(t‚Çñ)Œæ·µñ(t‚Çñ)·µÄ) = Q‚Çñ.

Thus, the goal of the filtering problem is to find a good estimate (analysis) u‚Çê(t‚Çñ) of the true system‚Äôs state u‚Çú(t‚Çñ) 
given a dynamical model and a set of noisy observations {w(t·µ¢)} collected at some time instants t·µ¢ ‚àà (0, t‚Çñ]. 
The optimality of the estimate u‚Çê(t‚Çñ) is defined as the one which minimizes E[(u‚Çú(t‚Çñ) ‚àí u‚Çê(t‚Çñ))·µÄ(u‚Çú(t‚Çñ) ‚àí u‚Çê(t‚Çñ))]. 
This filtering process generally consists of two steps: the forecast step and the data assimilation step.

The forecast step is performed using the predictable part of the given dynamical model starting 
from the best known information at time t‚Çñ (denoted as ≈´·µá(t‚Çñ)) to produce a forecast or background 
estimate ≈´(t‚Çñ+1) = M‚Çñ≈´·µá(t‚Çñ). 

The difference between the background forecast and true state at t‚Çñ+1 can be written as follows,
```
Œæ·µá(t‚Çñ‚Çä‚ÇÅ) = u‚Çú(t‚Çñ‚Çä‚ÇÅ) ‚àí ≈´·µá(t‚Çñ‚Çä‚ÇÅ)
           = (M‚Çñu‚Çú(t‚Çñ) + Œæ·µñ(t‚Çñ‚Çä‚ÇÅ)) ‚àí M‚Çñ≈´·µá(t‚Çñ)
           = M‚Çñ(u‚Çú(t‚Çñ) ‚àí ≈´·µá·µá(t‚Çñ)) + Œæ·µñ(t‚Çñ‚Çä‚ÇÅ)
           = M‚ÇñŒæ·µá(t‚Çñ) + Œæ·µñ(t‚Çñ‚Çä‚ÇÅ),

```
where Œæ·µá(t‚Çñ) = (u‚Çú(t‚Çñ) ‚àí ≈´·µá·µá(t‚Çñ)) is the error estimate at t‚Çñ, with zero mean and covariance matrix of B·µá‚Çñ.
The covariance matrix of the background estimate at t‚Çñ‚Çä‚ÇÅ can be evaluated as B‚Çñ‚Çä‚ÇÅ =
E[Œæ·µá(t‚Çñ‚Çä‚ÇÅ)Œæ·µá(t‚Çñ‚Çä‚ÇÅ)·µÄ] = E[(M‚ÇñŒæ·µá(t‚Çñ) + Œæ·µñ(t‚Çñ‚Çä‚ÇÅ))(M‚ÇñŒæ·µá(t‚Çñ) + Œæ·µñ(t‚Çñ‚Çä‚ÇÅ))·µÄ].

Since Œæ·µá(t‚Çñ) and Œæ·µñ(t‚Çñ‚Çä‚ÇÅ) are assumed to be uncorrelated (i.e., E[Œæ·µá(t‚Çñ)Œæ·µñ(t‚Çñ‚Çä‚ÇÅ)·µÄ] = 0), the background covariance matrix at t‚Çñ‚Çä‚ÇÅ can be computed as follows:
B‚Çñ‚Çä‚ÇÅ = M‚ÇñB·µá‚ÇñM‚Çñ·µÄ + Q‚Çñ‚Çä‚ÇÅ.

Now, with the forecast step, we have a background estimate at t‚Çñ‚Çä‚ÇÅ defined as ≈´·µá(t‚Çñ‚Çä‚ÇÅ) with a covariance matrix B‚Çñ‚Çä‚ÇÅ. 
Then, measurements w(t‚Çñ‚Çä‚ÇÅ) are collected at t‚Çñ‚Çä‚ÇÅ with a linear operator H‚Çñ‚Çä‚ÇÅ and measurement noise Œæ·µê(t‚Çñ‚Çä‚ÇÅ) 
with zero mean and a covariance matrix of R‚Çñ‚Çä‚ÇÅ. Thus, we would like to fuse these pieces of information to create an optimal 
unbiased estimate (analysis) u‚Çê(t‚Çñ‚Çä‚ÇÅ) with a covariance matrix P‚Çñ‚Çä‚ÇÅ. This can be defined as a linear function of ≈´·µá(t‚Çñ‚Çä‚ÇÅ) 
and w(t‚Çñ‚Çä‚ÇÅ) as follows,


```
u‚Çê(t‚Çñ‚Çä‚ÇÅ) = ≈´·µá(t‚Çñ‚Çä‚ÇÅ) + K‚Çñ‚Çä‚ÇÅ [w(t‚Çñ‚Çä‚ÇÅ) ‚àí H‚Çñ‚Çä‚ÇÅ≈´·µá(t‚Çñ‚Çä‚ÇÅ)]
```

where [w(t‚Çñ‚Çä‚ÇÅ) ‚àí H‚Çñ‚Çä‚ÇÅ≈´·µá(t‚Çñ‚Çä‚ÇÅ)] is the innovation vector and K ‚àà ‚Ñù‚ÅøÀ£·µê is called the Kalman gain matrix. 
We highlight that the Kalman gain matrix is defined in such a way to minimize E[(u‚Çú(t‚Çñ‚Çä‚ÇÅ) ‚àí u‚Çê(t‚Çñ‚Çä‚ÇÅ))·µÄ(u‚Çú(t‚Çñ‚Çä‚ÇÅ) ‚àí u‚Çê(t‚Çñ‚Çä‚ÇÅ))] = tr(P‚Çñ‚Çä‚ÇÅ).

This can be written as

```
K‚Çñ‚Çä‚ÇÅ = B‚Çñ‚Çä‚ÇÅH‚Çñ‚Çä‚ÇÅ·µÄ [H‚Çñ‚Çä‚ÇÅB‚Çñ‚Çä‚ÇÅH‚Çñ‚Çä‚ÇÅ·µÄ + R‚Çñ‚Çä‚ÇÅ]‚Åª¬π,
```
resulting in an analysis covariance matrix defined as
```
P‚Çñ‚Çä‚ÇÅ = (I‚Çô ‚àí K‚Çñ‚Çä‚ÇÅH‚Çñ‚Çä‚ÇÅ)B‚Çñ‚Çä‚ÇÅ,
```
where I‚Çô is the n √ó n identity matrix. The resulting analysis u‚Çê(t‚Çñ‚Çä‚ÇÅ) is known as the best linear unbiased estimate (BLUE). 
We highlight that information at t‚Çñ might correspond to the analysis (i.e., ≈´·µá·µá(t‚Çñ) = u‚Çê(t‚Çñ)) obtained from the last data 
assimilation implementation, or just from the previous forecast if no other information is available. Thus, the Kalman filtering process 
can be summarized as follows:
```
Inputs: ≈´·µá·µá(t‚Çñ), B·µá‚Çñ, M‚Çñ, Q‚Çñ‚Çä‚ÇÅ, w(t‚Çñ‚Çä‚ÇÅ), R‚Çñ‚Çä‚ÇÅ, H‚Çñ‚Çä‚ÇÅ

Forecast: ≈´·µá(t‚Çñ‚Çä‚ÇÅ) = M‚Çñ≈´·µá·µá(t‚Çñ)
B‚Çñ‚Çä‚ÇÅ = M‚ÇñB·µá‚ÇñM‚Çñ·µÄ + Q‚Çñ‚Çä‚ÇÅ

Kalman gain: K‚Çñ‚Çä‚ÇÅ = B‚Çñ‚Çä‚ÇÅH‚Çñ‚Çä‚ÇÅ·µÄ [H‚Çñ‚Çä‚ÇÅB‚Çñ‚Çä‚ÇÅH‚Çñ‚Çä‚ÇÅ·µÄ + R‚Çñ‚Çä‚ÇÅ]‚Åª¬π

Analysis: u‚Çê(t‚Çñ‚Çä‚ÇÅ) = ≈´·µá(t‚Çñ‚Çä‚ÇÅ) + K‚Çñ‚Çä‚ÇÅ [w(t‚Çñ‚Çä‚ÇÅ) ‚àí H‚Çñ‚Çä‚ÇÅ≈´·µá(t‚Çñ‚Çä‚ÇÅ)]
P‚Çñ‚Çä‚ÇÅ = (I‚Çô ‚àí K‚Çñ‚Çä‚ÇÅH‚Çñ‚Çä‚ÇÅ)B‚Çñ‚Çä‚ÇÅ,

where the inputs at t‚Çñ are defined as
(≈´·µá·µá(t‚Çñ), B·µá‚Çñ) = 
(u‚Çê(t‚Çñ), P‚Çñ) if w(t‚Çñ) is available,
(≈´·µá(t‚Çñ), B‚Çñ) otherwise.
```

The Kalman filter is a recursive algorithm that can be implemented in a sequential manner.

<h2 align=center> üìÉ Script Explanation </h2>
  <p align="center">

**DATA ASSIMILATION**
-----------------------------------------------------------------------------------------------------------------
**Step 1: Install Dependencies**
Since this is a simple data assimilation, for the first part of generating the analysis field, we just need numpy. If you dont have numpy installed on your PC, you can simply copy and paste the line of code below to install it.
```
pip install numpy
```

**Step 2: Set the observations and the Model Data**
The observations and the model_data below are sample air temperature data

```
import numpy as np

observations = np.array([15.2,16.1,14.5,15.8,25.0])
model_data = np.array([14.8,15.5,14.0,16.0,25.3])
```

**Step 3: Set the initial estimates**
The next step is to set the initial estimates for the state and the uncertainty of the state. Here we set the inital state estimate to be the first value of the model data and out of our choosing, we set the initial covariance estimate to be 1
E.g.
```
initial_state_estimate = model_data[0]
initial_covariance_estimate = 1
```

**Step 4: Set the Observational and Model Uncertainty**
The observation and model uncertainty are set to 1 and they correspond to the measurement_noise and the process_noise. Here what we are saying that maybe when the observational data was being taken, something happened and since we dont know the uncertainty, we equate it to 1. The same goes for the model
```
process_noise = 1
measurement_noise = 1

```




**Step 5: Create empty arrays and set initial values**
The next step is to create empty arrays to store the state estimates and the covariance estimates.

```
num_measurements = len(observations)

# Arrays to store the results
filtered_state_estimates = np.zeros(num_measurements)
filtered_covariance_estimates = np.zeros(num_measurements)

# Initial values
filtered_state_estimates[0] = initial_state_estimate
filtered_covariance_estimates[0] = initial_covariance_estimate

```

**Step 6: Loop through the number of observations**
The next step is to loop through the number of observations and update the state and the covariance estimates 
and also calculate the kalman gain for each step. The steps here are 1 to 5

Predicted State Estimate 
The predicted state estimate is a forecast of the state variable before incorporating the new observation at time step ùëò
It is based on the previous state estimate and the system's model, and it provides a means to anticipate the state of the system using all the information available up to the previous time step.

Significance: The predicted state estimate represents the expected state of the system before considering the new measurement. It is crucial for understanding how the system evolves over time based on the internal dynamics or model.

Predicted Covariance Estimate 
The predicted covariance estimate quantifies the uncertainty or confidence in the predicted state estimate. It combines the previous uncertainty with the process noise to reflect the increasing uncertainty over time due to the inherent variability in the system.

Significance: The predicted covariance estimate provides a measure of how much uncertainty there is in the predicted state. A larger predicted covariance implies greater uncertainty, while a smaller one implies more confidence in the prediction. This estimate is critical for determining the Kalman gain, which balances the reliance on the predicted state versus the new observation during the update step.

```
for k in range(1, num_measurements):
    # Prediction step
    predicted_state_estimate = filtered_state_estimates[k-1]
    predicted_covariance_estimate = filtered_covariance_estimates[k-1] + process_noise

    # Update step
    kalman_gain = predicted_covariance_estimate / (predicted_covariance_estimate + measurement_noise)
    filtered_state_estimates[k] = predicted_state_estimate + kalman_gain * (observations[k] - predicted_state_estimate)
    filtered_covariance_estimates[k] = (1 - kalman_gain) * predicted_covariance_estimate
    

```

Assimilated Data: The filtered_state_estimates[k] represents the best estimate of the state at time ùëò
k after assimilating both the model prediction and the observational data. It leverages the strength of both sources: the temporal consistency of the model and the accuracy of the observations.
Reduced Uncertainty: Through the Kalman gain, the filter adjusts the influence of the prediction and the observation based on their uncertainties. If the model prediction is more certain (lower predicted covariance), it will have more influence. Conversely, if the observation is more certain (lower measurement noise), it will have more influence.
Continuous Adjustment: At each time step, the filter continuously refines its state estimate, ensuring that the assimilated data reflects both the latest observation and the underlying model dynamics.


**ANALYSIS AND COMPARISONS**
----------------------------------------------------------------------------------------------------------------
We are to compare the analysis fields, which are the best estimates (filtered state estimates) with the observations and the NWP model output and evaluate the improvement made by the data assimilation process by using statistical metrics such as RMSE and Bias.

**Step 1: Import packages**
The first step is to import the necessary python packages for the statistics and the plotting packages.
Again, if you dont have it installed on your PC, go ahead and install it but if you have them installed, just import them like the ones here below.
```
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

```

**Step 2: Make a dataframe out of the Observations, Model and the Assimilated (Filtered States Estimates)**
```
data = {
    "Model Data": model_data,
    "Observations": observations,
    "Assimilated Data": filtered_state_estimates
}
df = pd.DataFrame(data)
df

```

**Step 3: Compute RMSE and BIAS**
The next step is to calculate the root mean square error and the bias for the data. These are statistical metrics that helps us to find the performance of made by the assimilated data.
```
RMSE_o = mean_squared_error(df['Observations'], df['Assimilated Data'],squared=False)
MBE_o = np.mean(df['Assimilated Data'] - df['Observations'])

RMSE_m = mean_squared_error(df['Model Data'], df['Assimilated Data'],squared=False)
MBE_m = np.mean(df['Assimilated Data'] - df['Model Data'])

```

**Step 4: Heatmap of the statistical metrics**
The final step is to make a visualization of the statistical metrics.
```
Observations_data = {
    "Observed":["RMSE", "MBE"],
    "Assimilated": ["RMSE", "MBE"],
    "Values": [RMSE_o, MBE_o]
}

Modelled_data = {
    "Modelled":["RMSE", "MBE"],
    "Assimilated": ["RMSE", "MBE"],
    "Values": [RMSE_m, MBE_m]
}


df1 = pd.DataFrame(Observations_data)
df2 = pd.DataFrame(Modelled_data)


fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,8))
fig.subplots_adjust(wspace=0.04)

sns.heatmap(df1.pivot("Observed", "Assimilated", values="Values"),annot=True,annot_kws={'fontsize':15}, ax=ax1)
sns.heatmap(df2.pivot("Modelled", "Assimilated", values="Values"),annot=True,annot_kws={'fontsize':15}, ax=ax2)


fig.suptitle("STATISTICAL METRICS TO QUANTIFY PERFORMANCE MADE BY THE DATA ASSIMILATION")

ax1.set_title("Observation", fontweight ="bold", fontsize=15)
ax1.tick_params(axis='both',labelsize=15)
ax1.set_xlabel('Assimilated', fontsize=15)
ax1.set_ylabel('Observed \n', fontsize=15)

ax2.set_title("Modelled", fontweight ="bold", fontsize=15)
ax2.tick_params(axis='both',labelsize=15)
ax2.set_xlabel('Assimilated', fontsize=15)
ax2.set_ylabel('Model', fontsize=15)

```

**Reference**
I took my reference from https://pdfs.semanticscholar.org/75d1/19f6ccbc3cc0dfa6ae0dc343803eeee897e2.pdf